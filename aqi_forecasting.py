# -*- coding: utf-8 -*-
"""AQI Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jtATXJYcK0LOncZwN_VXxyY5fr2SOs7N
"""

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
beijing_pm2_5 = fetch_ucirepo(id=381)

# data (as pandas dataframes)
X = beijing_pm2_5.data.features
y = beijing_pm2_5.data.targets

# metadata
print(beijing_pm2_5.metadata)

# variable information
print(beijing_pm2_5.variables)

"""Additional Information on Variables

No: row number;
year: year of data in this row;
month: month of data in this row;
day: day of data in this row;
hour: hour of data in this row;
pm2.5: PM2.5 concentration (ug/m^3);
DEWP: Dew Point (â„ƒ);
TEMP: Temperature (â„ƒ);
PRES: Pressure (hPa);
cbwd: Combined wind direction;
Iws: Cumulated wind speed (m/s);
Is: Cumulated hours of snow;
Ir: Cumulated hours of rain;

Exploratory Data Analysis on Beijing PM2.5 dataset
"""

# Basic info about the dataset
print(X.info())

print(X.head())  # First few rows of features

print(y.head())  # First few rows of target variable (PM2.5 values)

print(X.isnull().sum())  # Number of missing values in each feature

print(y.isnull().sum())  # Missing values in the target variable

# Fetch the dataset
from ucimlrepo import fetch_ucirepo
import pandas as pd


# Fetch dataset
beijing_pm2_5 = fetch_ucirepo(id=381)

# Data (as pandas dataframes)
X = beijing_pm2_5.data.features
y = beijing_pm2_5.data.targets

# Creating a new column for the target variable 'PM2.5'
X['PM2.5'] = y

# Creating datetime index for better visualization
X['date'] = pd.to_datetime(X[['year', 'month', 'day', 'hour']])
X.set_index('date', inplace=True)

# Print first 10 rows of the combined DataFrame
print(X.head(10))

import pandas as pd

# Load the dataset
from ucimlrepo import fetch_ucirepo
beijing_pm2_5 = fetch_ucirepo(id=381)
X = beijing_pm2_5.data.features
y = beijing_pm2_5.data.targets

# Print unique values of the 'cbwd' column
unique_cbwd_values = X['cbwd'].unique()
print("Unique values in 'cbwd':", unique_cbwd_values)

import pandas as pd

# Load the dataset
from ucimlrepo import fetch_ucirepo
beijing_pm2_5 = fetch_ucirepo(id=381)
X = beijing_pm2_5.data.features
y = beijing_pm2_5.data.targets

X['cbwd'] = X['cbwd'].replace('cv', 'SW')

# Combine features and target into one DataFrame
df = pd.concat([X, y], axis=1)

# Convert categorical column 'cbwd' into numerical format using One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=['cbwd'])

# Separate features and target again
X = df_encoded.drop(columns=['pm2.5'])
y = df_encoded['pm2.5']

# Print the updated DataFrame
print(df_encoded.head())

import seaborn as sns
import matplotlib.pyplot as plt
# Compute the correlation matrix
correlation_matrix = df_encoded.corr()


# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.3f')
plt.title('Correlation Matrix')
plt.show()

# Summary statistics of the features
print(X.describe())

# Summary statistics of the target (PM2.5)
print(y.describe())

# Combine the time components into a single datetime index
X['date'] = pd.to_datetime(X[['year', 'month', 'day', 'hour']])
X.set_index('date', inplace=True)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.scatter(X.index, y, label='PM2.5 concentration', alpha=0.3)
plt.xlabel('Date')
plt.ylabel('PM2.5 Level')
plt.title('PM2.5 Concentration Over Time')
plt.legend()
plt.show()

# Plotting distributions for some important features
important_features = ['DEWP', 'TEMP', 'PRES', 'Iws']  # Dew Point, Temperature, Pressure, Wind Speed

for feature in important_features:
    plt.figure(figsize=(6, 4))
    sns.histplot(X[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.impute import KNNImputer
from sklearn.metrics import mean_squared_error


# Combine features and target into one DataFrame
df = pd.concat([X, y], axis=1)

# Separate features and target
X = df_encoded.drop(columns=['pm2.5'])
y = df_encoded['pm2.5']

# Create a DataFrame with missing values for imputation
df_missing = df_encoded.copy()
df_missing['pm2.5'] = df_missing['pm2.5'].replace(0, np.nan)  # Assuming 0 represents missing values

# Prepare data for imputation
X_missing = df_missing.drop(columns=['pm2.5'])
y_missing = df_missing['pm2.5']

# 1. Interpolation
df_interp = df_encoded.copy()
df_interp['pm2.5'] = df_interp['pm2.5'].interpolate(method='linear', limit_direction='both')

# 2. SVM Regression
svm = SVR()
X_train_svm = X[~y.isnull()]
y_train_svm = y.dropna()
svm.fit(X_train_svm, y_train_svm)
svm_pred = svm.predict(X_missing)

# Ensure that svm_pred length matches the missing values
df_svm = df_encoded.copy()
df_svm.loc[df_svm['pm2.5'].isnull(), 'pm2.5'] = svm_pred[:df_svm['pm2.5'].isnull().sum()]

# 3. KNN Imputation
knn_imputer = KNNImputer(n_neighbors=5)
X_imputed_knn = knn_imputer.fit_transform(X_missing)
df_knn = df_encoded.copy()
df_knn['pm2.5'] = knn_imputer.fit_transform(X_missing)[:, 0]

# 4. Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_svm, y_train_svm)
rf_pred = rf.predict(X_missing)

# Ensure that rf_pred length matches the missing values
df_rf = df_encoded.copy()
df_rf.loc[df_rf['pm2.5'].isnull(), 'pm2.5'] = rf_pred[:df_rf['pm2.5'].isnull().sum()]

# 5. Gradient Boosting Regressor
gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)
gbr.fit(X_train_svm, y_train_svm)
gbr_pred = gbr.predict(X_missing)

# Ensure that gbr_pred length matches the missing values
df_gbr = df_encoded.copy()
df_gbr.loc[df_gbr['pm2.5'].isnull(), 'pm2.5'] = gbr_pred[:df_gbr['pm2.5'].isnull().sum()]

import matplotlib.pyplot as plt

# Set up the plot figure with a larger size
plt.figure(figsize=(15, 10))

# Subplot 1: Original vs Interpolated Values
plt.subplot(2, 3, 1)
plt.scatter(df_encoded.index, df_encoded['pm2.5'], label='Original Values', s=3, color='blue')
plt.title('Original')
plt.xlabel('Index')
plt.ylabel('PM2.5')
plt.legend()

# Subplot 2: Interpolated Values Only
plt.subplot(2, 3, 2)
#plt.scatter(df_encoded.index, df_encoded['pm2.5'], label='Original Values', s=3, color='blue')
plt.scatter(df_interp.index, df_interp['pm2.5'], label='Interpolated Values', s=3, color='red')
plt.title('Interpolated Values')
plt.xlabel('Index')
plt.ylabel('PM2.5')
plt.legend()

# Subplot 3: Original vs SVM Imputed Values
plt.subplot(2, 3, 3)
#plt.scatter(df_encoded.index, df_encoded['pm2.5'], label='Original Values', s=3, color='blue')
plt.scatter(df_svm.index, df_svm['pm2.5'], label='SVM Imputed Values', s=3, color='green')
plt.title('SVM Imputed Values')
plt.xlabel('Index')
plt.ylabel('PM2.5')
plt.legend()

# Subplot 4: Original vs KNN Imputed Values
plt.subplot(2, 3, 4)
#plt.scatter(df_encoded.index, df_encoded['pm2.5'], label='Original Values', s=3, color='blue')
plt.scatter(df_knn.index, df_knn['pm2.5'], label='KNN Imputed Values', s=3, color='orange')
plt.title('KNN Imputed Values')
plt.xlabel('Index')
plt.ylabel('PM2.5')
plt.legend()

# Subplot 5: Original vs Random Forest Imputed Values
plt.subplot(2, 3, 5)
#plt.scatter(df_encoded.index, df_encoded['pm2.5'], label='Original Values', s=3, color='blue')
plt.scatter(df_rf.index, df_rf['pm2.5'], label='RF Imputed Values', s=3, color='purple')
plt.title('Random Forest Imputed Values')
plt.xlabel('Index')
plt.ylabel('PM2.5')
plt.legend()

# Subplot 6: Original vs Gradient Boosting Imputed Values
plt.subplot(2, 3, 6)
#plt.scatter(df_encoded.index, df_encoded['pm2.5'], label='Original Values', s=3, color='blue')
plt.scatter(df_gbr.index, df_gbr['pm2.5'], label='GB Imputed Values', s=3, color='brown')
plt.title('Gradient Boosting Imputed Values')
plt.xlabel('Index')
plt.ylabel('PM2.5')
plt.legend()

# Adjust layout to prevent overlapping
plt.tight_layout()

# Show the final plot
plt.show()

"""Outlier Detection

"""

df=df_interp

df.index.name = 'Index'
df.to_csv('preprocessed_data_2.csv', index=True, header=True,columns=['year', 'month', 'day', 'hour', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is',
       'Ir', 'pm2.5', 'cbwd_NE', 'cbwd_NW', 'cbwd_SE', 'cbwd_SW'])

import pandas as pd

# Step 1: Separate numerical columns (excluding categorical ones)
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Displaying the columns being checked for outliers
print(f"Numerical columns being analyzed: {numerical_columns}")

# Step 2: Function to detect outliers using the IQR method
def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)  # First quartile (25th percentile)
    Q3 = df[column].quantile(0.75)  # Third quartile (75th percentile)
    IQR = Q3 - Q1  # Interquartile range

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identifying outliers
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Step 3: Check for outliers in each numerical column
outliers_dict = {}
for column in numerical_columns:
    outliers = detect_outliers_iqr(df, column)
    outliers_dict[column] = outliers
    print(f"\nNumber of outliers in column '{column}': {len(outliers)}")

# Step 4: Display or store the outlier records
for column, outliers in outliers_dict.items():
    if len(outliers) > 0:
        print(f"\nOutliers in column '{column}':")
        print(outliers[[column]])  # Display only the outlier rows for the column

import matplotlib.pyplot as plt
import seaborn as sns

# Set the figure size for better visualization
plt.figure(figsize=(15, 10))

# Step 1: Separate numerical columns (excluding categorical ones)
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Step 2: Plot boxplots for each numerical column
for i, column in enumerate(numerical_columns, 1):
    plt.subplot(len(numerical_columns)//3 + 1, 3, i)  # Create subplots
    sns.boxplot(y=df[column])
    plt.title(f'Box plot for {column}')

# Step 3: Adjust layout and show the plot
plt.tight_layout()
plt.show()

from sklearn.preprocessing import RobustScaler

# Separate numerical columns from the dataframe (excluding categorical columns)
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Initialize RobustScaler
scaler = RobustScaler()

# Apply the scaler to the numerical columns only
df_scaled = df.copy()  # Create a copy to retain original data
df_scaled[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Show the first few rows of the scaled dataframe
print(df_scaled.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Set the figure size for better visualization
plt.figure(figsize=(15, 10))

# Step 1: Separate numerical columns (excluding categorical ones)
numerical_columns = df_scaled.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Step 2: Plot boxplots for each numerical column
for i, column in enumerate(numerical_columns, 1):
    plt.subplot(len(numerical_columns)//3 + 1, 3, i)  # Create subplots
    sns.boxplot(y=df_scaled[column])
    plt.title(f'Box plot for {column}')

# Step 3: Adjust layout and show the plot
plt.tight_layout()
plt.show()

df_scaled.shape



"""Forecasting using LSTMs/GRUs"""

import numpy as np
import pandas as pd

# Separate features and target
features = df_scaled.drop(columns=['pm2.5']).values  # 14 feature columns
target = df_scaled['pm2.5'].values  # Target column

# Define function to create sequences of input features and labels
def create_sequences(features, target, n_steps):
    X, y = [], []
    for i in range(n_steps, len(features)):
        X.append(features[i-n_steps:i, :])  # last n_steps rows of feature columns
        y.append(target[i])  # corresponding pm2.5 value
    return np.array(X), np.array(y)

# Set number of time steps (window size)
n_steps = 60  # You can experiment with this value

# Prepare input sequences
X, y = create_sequences(features, target, n_steps)

# Check the shape of X to ensure it matches the required shape for LSTM (samples, time steps, features)
print("X shape:", X.shape)  # Should be (samples, 60, 14)
print("y shape:", y.shape)  # Should be (samples,)

# Split into training and testing sets (80% train, 20% test)
train_size = int(X.shape[0] * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

!pip install tensorflow

# Ensure all data is in float32 format
X_train = np.array(X_train, dtype=np.float32)
X_test = np.array(X_test, dtype=np.float32)
y_train = np.array(y_train, dtype=np.float32)
y_test = np.array(y_test, dtype=np.float32)

# Check for any NaN or infinite values in the dataset
print("NaNs in X_train:", np.isnan(X_train).sum())
print("NaNs in y_train:", np.isnan(y_train).sum())
print("NaNs in X_test:", np.isnan(X_test).sum())
print("NaNs in y_test:", np.isnan(y_test).sum())

print("Infs in X_train:", np.isinf(X_train).sum())
print("Infs in y_train:", np.isinf(y_train).sum())
print("Infs in X_test:", np.isinf(X_test).sum())
print("Infs in y_test:", np.isinf(y_test).sum())

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, GRU

# Initialize the LSTM/GRU model
model = Sequential()

# Add LSTM layer (or GRU layer)
model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, X.shape[2])))
model.add(LSTM(50, activation='relu'))
model.add(Dense(1))  # Output layer for predicting PM2.5

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# Make predictions on the test set
y_pred = model.predict(X_test)

# Invert scaling if necessary (since we applied RobustScaler earlier)
# If PM2.5 column was scaled, use inverse_transform to rescale predictions and true values
# y_pred_rescaled = scaler.inverse_transform(y_pred)

# Plot true vs predicted values
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(y_test, label='True PM2.5')
plt.plot(y_pred, label='Predicted PM2.5', color='red')
plt.title('PM2.5 Forecasting using LSTM/GRU')
plt.xlabel('Time')
plt.ylabel('PM2.5')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error

# Calculate MSE and MAE
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f'MSE: {mse}, MAE: {mae}')